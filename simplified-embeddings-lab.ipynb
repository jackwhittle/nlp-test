{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embeddings: Understanding Word Context\n",
    "\n",
    "### About:  \n",
    "Learn how to convert text into numerical vectors while preserving meaning and context. We'll explore how modern language models like BERT handle word meanings differently based on their context.\n",
    "\n",
    "### Prerequisites:\n",
    "- Basic Python knowledge\n",
    "- pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Understanding Context\n",
    "\n",
    "Let's look at how the same word can have different meanings based on context:\n",
    "\n",
    "1. \"The supplier agreed to pay the manufacturer\"\n",
    "2. \"The manufacturer agreed to pay the supplier\"\n",
    "\n",
    "While these sentences use the same words, their meanings are quite different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our test sentences\n",
    "sentence1 = \"The supplier agreed to pay the manufacturer\"\n",
    "sentence2 = \"The manufacturer agreed to pay the supplier\"\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embeddings for a sentence\n",
    "def get_bert_embedding(sentence):\n",
    "    # Tokenize and get model outputs\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get embeddings from last hidden state\n",
    "    embeddings = outputs.last_hidden_state[0].detach().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    return embeddings, tokens\n",
    "\n",
    "# Get embeddings for both sentences\n",
    "emb1, tokens1 = get_bert_embedding(sentence1)\n",
    "emb2, tokens2 = get_bert_embedding(sentence2)\n",
    "\n",
    "# Print the tokens to see how BERT breaks down our sentences\n",
    "print(\"Sentence 1 tokens:\", tokens1)\n",
    "print(\"Sentence 2 tokens:\", tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways:\n",
    "\n",
    "1. Notice that 'supplier' and 'manufacturer' have different embeddings depending on their role in the sentence (payer vs payee)\n",
    "2. The word 'pay' has very similar embeddings because it plays the same role in both sentences\n",
    "3. This demonstrates how modern language models like BERT understand context, not just individual words\n",
    "\n",
    "### Try it yourself!\n",
    "Try creating your own pair of sentences that use the same words in different contexts and see how their embeddings compare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
