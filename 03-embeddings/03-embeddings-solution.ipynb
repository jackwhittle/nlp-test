{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Embeddings\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About:  \n",
    "In this notebook you will compare two embedding methods, Word2Vec and BERT focusing on how each model represents text as vectors. \n",
    "\n",
    "\n",
    "### Learning Objective:\n",
    "- Compare two embedding methods, Word2Vec and BERT, using customer reviews.\n",
    "\n",
    "### Installs\n",
    "- [gensim](https://radimrehurek.com/gensim/intro.html#installation)\n",
    "\n",
    "\n",
    "### Notebook Guide\n",
    "\n",
    "- NLP Scenario\n",
    "- Static Embeddings\n",
    "- Contextual Embeddings\n",
    "- Conclusions and Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Gensim for word2vec \n",
    "# to clean text and implement word2vec imports \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "## Pytorch imports for BERT embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "## Cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Scenario \n",
    "You are an analyst for a marketing company that just launched a new product suite of mobile devices. You have data from product reviews of the new product. In this notebook your goal is to evaluate different methods for representing text with text embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Product Reviews \n",
    "1. \"I absolutely love the TechWave X1! It has made my daily tasks so much easier and more efficient. Highly recommend it!\"\n",
    "2. \"I'm not very impressed with the TechWave X1. It lacks some essential features and is quite slow.\"\n",
    "3. \"The TechWave X1 is fantastic! It has exceeded my expectations and has become an essential part of my daily routine.\"\n",
    "4. \"I found the TechWave X1 to be quite average. It does the job, but there's nothing particularly special about it.\"\n",
    "5. \"The TechWave X1 is terrible. It's full of glitches and crashes frequently. I regret purchasing it.\"\n",
    "6.  \"The TechWave X1 is disappointing. It doesn't live up to the hype and is missing several key functionalities.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data setup \n",
    "# create a list of reviews\n",
    "\n",
    "reviews = [\"I absolutely love the TechWave X1! It has made my daily tasks so much easier and more efficient. Highly recommend it!\",\n",
    "\"I'm not very impressed with the TechWave X1. It lacks some essential features and is quite slow.\",\n",
    "\"The TechWave X1 is fantastic! It has exceeded my expectations and has become an essential part of my daily routine.\",\n",
    "\"I found the TechWave X1 to be quite average. It does the job, but there's nothing particularly special about it.\",\n",
    "\"The TechWave X1 is terrible. It's full of glitches and crashes frequently. I regret purchasing it.\",\n",
    "\"The TechWave X1 is disappointing. It doesn't live up to the hype and is missing several key functionalities.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Embeddings \n",
    "\n",
    "We will use the `Gensim` implementation of `Word2Vec` on our reviews to explore static embeddings. With static embeddings  a given word will have a set vector regardless of the words around it. \n",
    "\n",
    "\n",
    "`Gensim` is a popular open-source library for natural language processing. It excels at topic modeling and similarity methods using models such as `Latent Dirichlet Allocation (LDA)` and `Word2Vec`. We will use `Word2Vec` to find the similarity across our reviews. \n",
    "\n",
    "### Tokenization and pre-processing\n",
    "In order to use the `Word2Vec model` we first need to convert our raw text to tokens. \n",
    "\n",
    "We will do this by defining a function called `preprocess_text` that uses built-in preprocessing tools from `Gensim` to:\n",
    "1. make all the words lowercase \n",
    "2. remove special characters \n",
    "3. remove very common words, known as stop words \n",
    "\n",
    "After running this function we will create a list that contains cleaned tokens for each review. These tokens will be used in our `Word2Vec` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and tokenize text using gensim's preprocessing utilities\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and complete other basic cleaning \n",
    "    tokens = simple_preprocess(text)  \n",
    "\n",
    "    # Remove stopwords\n",
    "    clean_text = remove_stopwords(' '.join(tokens))\n",
    "    tokens = clean_text.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the reviews by calling our preprocess_text function on each review and saving to a new list\n",
    "\n",
    "processed_reviews = [preprocess_text(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review our tokenized reviews\n",
    "\n",
    "for review in processed_reviews:\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the tokenized reviews with the original reviews. Note that the text has been converted to lowercase; special characters and numbers have been removed; and stopwords have also been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing `Word2Vec`\n",
    "\n",
    "Text embeddings aim to preserve the meaning and context of words by assigning a vector to words where similar words with similar meanings or contexts would have similar vectors. \n",
    "\n",
    "We will call [Gensims's Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) to demonstrate one example of a static embedding. \n",
    "- Input: \n",
    "  - Tokens for each review that have been pre-processed (cleaned)\n",
    "- Output:\n",
    "  - A vector (size = 100 here) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the word2vec model on our reviews\n",
    "w2v_model = Word2Vec(sentences=processed_reviews, vector_size=100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word vector for a word ('impressed')\n",
    "w2v_model.wv['impressed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and another word ('terrible')\n",
    "w2v_model.wv['terrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most similar words to a word\n",
    "w2v_model.wv.most_similar('impressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('terrible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the word 'impressed' is similar to 'essential' and 'fantastic', while the word 'terrible' is similar to 'hype' and 'disappointing'. Note some words are similar to both (such as 'easier')! \n",
    "\n",
    "This shows that the `Word2Vec` model has learned the some of the semantic relationships between words based on the context in which they appear in the reviews. Given our very small training set, these results are impressive. With more data and more examples of usage for the model to train on, these results would improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it! \n",
    "### Evaluate the impact of modifying your vector size \n",
    "\n",
    "`Word2Vec` has multiple parameters that you can modify, and that will change the results of your model.\n",
    "\n",
    "Below are a few key parameters you may want to edit, and their default values. \n",
    "\n",
    "- vector_size: int = 100\n",
    "  - [Vector size docs](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#vector-size)\n",
    "- window: int = 5 \n",
    "  -  number of words used ranges from 2-10 \n",
    "- min_count: int = 5\n",
    "  - [Min Count Docs](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#min-count)\n",
    "\n",
    "#### Example with default values \n",
    "```python \n",
    "Word2Vec(sentences=processed_reviews, vector_size=100, window=5, min_count=5)\n",
    "``` \n",
    "\n",
    "Give it a try! Modify your `Word2Vec` parameters in each exercise below and observe your output.\n",
    "\n",
    "#### Smaller vector\n",
    "- Modify the `Word2Vec` model to use a smaller vector size (e.g. 10)\n",
    "- Then, retrain the model on the processed reviews and find the most similar words to the word \"impressed\".\n",
    "- What differences do you notice in the results compared to the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the word2vec model on our reviews with a smaller vector size\n",
    "w2v_model = Word2Vec(sentences=processed_reviews, vector_size=10, window=5, min_count=1)\n",
    "\n",
    "# Find the most similar words to 'impressed' \n",
    "w2v_model.wv.most_similar('impressed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Larger Vector and Window\n",
    "- Repeat the process and modify the `Word2Vec` model to use a larger vector size (e.g. 300) and a larger window size (e.g. 10). \n",
    "- What differences do you notice in the results compared to the previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Word2Vec model on our reviews with a smaller vector size\n",
    "w2v_model = Word2Vec(sentences=processed_reviews, vector_size=300, window=10, min_count=1)\n",
    "\n",
    "# Find the most similar words to 'impressed' \n",
    "w2v_model.wv.most_similar('impressed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `Word2Vec` model is sensitive to the vector size and other hyperparameters. These choices can affect the quality of the word embeddings and the similarity results. Results are also impacted by your training data.\n",
    "\n",
    "Here we only have a small set of reviews, so the word embeddings may not be as accurate as they would be with a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding word order \n",
    "\n",
    "Let's examine how word order affects meaning with these two sentences:\n",
    "\n",
    "- \"The supplier agreed to pay the manufacturer\"\n",
    "- \"The manufacturer agreed to pay the supplier\"\n",
    "\n",
    "The meaning is changed depending on which words come first. Let's first make embeddings using a static embedding (`Word2Vec`), then compare the difference when we use a contextual embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it!  \n",
    "Use the steps above to convert the following two sentences using `Word2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of sentences\n",
    "sentences = [\"The supplier agreed to pay the manufacturer\",\n",
    "             \"The manufacturer agreed to pay the supplier\"]\n",
    "\n",
    "\n",
    "# Preprocess the sentences using the function we created earlier, preprocess_text()\n",
    "processed_reviews = [preprocess_text(sentence) for sentence in sentences]\n",
    "\n",
    "# Initialize the Word2Vec model\n",
    "w2v_model= Word2Vec(sentences=processed_reviews, vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word vector for a word ('supplier')\n",
    "w2v_model.wv['supplier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word vector for a word ('manufacturer')\n",
    "w2v_model.wv['manufacturer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stretch: calculate the cosine similarity of the same words in two sentences (with two meanings)\n",
    "\n",
    "cos_sim = cosine_similarity([w2v_model.wv['supplier']], [w2v_model.wv['supplier']])\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did you notice? \n",
    "\n",
    "These two sentences would have identical representations in `Word2Vec` but have very different meanings. In this example, the order of the words is critical to the meaning of the phrase. We aren't able to capture this order with the bag-of-words approach used in `Word2Vec` and other similar models.\n",
    "\n",
    "To address this issue, methods were developed to capture the sequential order of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Embeddings \n",
    "\n",
    "Unlike static embeddings, such as `Word2Vec`, **contextual embeddings** create different embeddings for the same word depending on the context in which it is used. To capture the context we use information such as the words around our word of interest, its sentence position, and other factors to map its vector.\n",
    "\n",
    "**In other words, contextual embeddings are position aware.**\n",
    "\n",
    "Let's look at our example from before: \n",
    "- \"The supplier agreed to pay the manufacturer\"\n",
    "- \"The manufacturer agreed to pay the supplier\"\n",
    "\n",
    "We will use `BERT`, a famous language model that uses transformer encoder architecture, to create contextual embeddings.\n",
    "\n",
    "We can access this model using the [Transformers library in HuggingFace](https://huggingface.co/docs/transformers/en/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences\n",
    "sentence1 = \"The supplier agreed to pay the manufacturer\"\n",
    "sentence2 = \"The manufacturer agreed to pay the supplier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create our tokens.  \n",
    "_Note: **tensors** are a general term for algebraic objects including vectors, and matrices.  You can [learn more about tensors here](https://en.wikipedia.org/wiki/Tensor)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens and convert to a tensor \n",
    "input1 = tokenizer(sentence1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Take a look at the tokenized sentence\n",
    "tokenizer.convert_ids_to_tokens(input1['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `BERT` tokenizer added two special tokens:\n",
    "- `[CLS]` is BERT's way of marking the start of the input document (a sentence in this case)\n",
    "- `[SEP]` indicates a separation (such as between sentences) and the end of a document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it! \n",
    "Create the tokens for sentence 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens and convert to a tensor \n",
    "input2 = tokenizer(sentence2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Take a look at the tokenized sentence\n",
    "tokenizer.convert_ids_to_tokens(input2['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup code for BERT embeddings \n",
    "\n",
    "Let's define 2 functions to create the tokens and then use them to create the `BERT` embeddings at the sentence level and the token level. For now, focus on the outputs. \n",
    "\n",
    "How this code block works in detail is beyond the scope of this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get BERT embeddings and cosine similarity scores \n",
    "\n",
    "\n",
    "def get_bert_embedding(sentence):\n",
    "    \"\"\"\n",
    "    Get BERT embeddings for a sentence\n",
    "    \"\"\"\n",
    "    # Tokenize and get model outputs\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    # Get model outputs\n",
    "    outputs = bert_model(**inputs)\n",
    "    \n",
    "    # Get embeddings from last hidden state and convert to numpy\n",
    "    embeddings = outputs.last_hidden_state[0].detach().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    return embeddings, tokens\n",
    "\n",
    "def compare_word(word, emb1, tokens1, emb2, tokens2):\n",
    "    \"\"\"\n",
    "    Compare BERT embeddings for a specific word in two embeddings\n",
    "    \"\"\"\n",
    " \n",
    "    # Find the word indices\n",
    "    idx1 = tokens1.index(word)\n",
    "    idx2 = tokens2.index(word)\n",
    "    \n",
    "    # Get the word vectors\n",
    "    vec1 = emb1[idx1]\n",
    "    vec2 = emb2[idx2]\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0]\n",
    "    \n",
    "    print(f\"Comparing embeddings for word '{word}':\")\n",
    "    print(f\"Similarity score: {similarity:.4f}\")\n",
    "    print(f\"Vector in sentence 1 (first 5 values): {vec1[:5]}\")\n",
    "    print(f\"Vector in sentence 2 (first 5 values): {vec2[:5]}\")\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass each of our sentences to the function to get the embeddings for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each sentence as a reminder of what we are working with\n",
    "print(sentence1)\n",
    "print(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for both sentences\n",
    "emb1, tokens1 = get_bert_embedding(sentence1)\n",
    "emb2, tokens2 = get_bert_embedding(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the output for supplier \n",
    "tokens2.index('supplier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the cosine similarity scores for supplier in each sentence\n",
    "supplier_score = compare_word('supplier', emb1, tokens1, emb2, tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the cosine similarity scores for manufacturer in each sentence\n",
    "manufacturer_score = compare_word('manufacturer', emb1, tokens1, emb2, tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same word but different embeddings! That's because the order in which they were used (sequence) in each sentence was used in the embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it! \n",
    "Checkout the embeddings for another word of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_word('pay', emb1, tokens1, emb2, tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that \"pay\" has a very high similarity across the two sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Takeaways\n",
    "In this notebook, we learned how to use: \n",
    "1. Word embeddings to represent text data\n",
    "2. Word2Vec model to generate static word embeddings for a list of reviews\n",
    "   - Word2Vec is a static embedding that has the same numerical representation for a given word regardless of context \n",
    "   - Its performance varies based on how it is tuned and the data used for training \n",
    "3. Pre-trained BERT model to generate contextual embeddings for a pair of sentences.\n",
    "   - BERT is a contextual embedding that creates different embeddings for the same word depending on how it was used in a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended readings\n",
    "- [Foundational word2vec paper](https://arxiv.org/abs/1301.3781)\n",
    "- [Foundational BERT paper](https://arxiv.org/pdf/1810.04805)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
