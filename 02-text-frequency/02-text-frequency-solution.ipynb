{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Text Frequency\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### About:  \n",
    "This notebook explores CountVectorizer and TF-IDF and uses the results to predict customer star ratings based on their reviews of a product. This will serve as proxy for sentiment analysis. \n",
    "\n",
    "### Learning Objective:\n",
    "- Apply CountVectorizer and TF-IDF to complete a sentiment analysis of customer reviews.\n",
    "\n",
    "### Notebook Guide\n",
    "\n",
    "- NLP Scenario\n",
    "- Introduction\n",
    "- Stop words\n",
    "- N-gram Length\n",
    "- TF-IDF\n",
    "- Try It!\n",
    "- Conclusions and Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# model building imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to avoid truncation of the output below \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Scenario \n",
    "You are an analyst for a marketing company that just launched a new product suite of mobile devices. You have data from product reviews of the one of these new products, TechWave X1. For this exercise you will use the text from the reviews to predict customer sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Product Reviews \n",
    "1. \"I absolutely love the TechWave X1! It has made my daily tasks so much easier and more efficient. Highly recommend it!\"\n",
    "2. \"I'm not very impressed with the TechWave X1. It lacks some essential features and is quite slow.\"\n",
    "3. \"The TechWave X1 is fantastic! It has exceeded my expectations and has become an essential part of my daily routine.\"\n",
    "4. \"I found the TechWave X1 to be quite average. It does the job, but there's nothing particularly special about it.\"\n",
    "5. \"The TechWave X1 is terrible. It's full of glitches and crashes frequently. I regret purchasing it.\"\n",
    "6.  \"The TechWave X1 is disappointing. It doesn't live up to the hype and is missing several key functionalities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "Computers understand the world in numbers. In order to use text data we have to first represent it in a way that computers can interpret. \n",
    "\n",
    "Let's convert the text above to a data frame and start working with the text data. \n",
    "\n",
    "\n",
    "This is a toy example with a small data set from demo/walkthrough purposes, which is why we're manually converting it to a dataframe. In a real task/in later tasks you would import the data from csv or txt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data setup\n",
    "reviews_with_ratings = [\n",
    "    (\"I absolutely love the TechWave X1! It has made my daily tasks so much easier and more efficient. Highly recommend it!\", 5),\n",
    "    (\"I'm not very impressed with the TechWave X1. It lacks some essential features and is quite slow.\", 2),\n",
    "    (\"The TechWave X1 is fantastic! It has exceeded my expectations and has become an essential part of my daily routine.\", 5),\n",
    "    (\"I found the TechWave X1 to be quite average. It does the job, but there's nothing particularly special about it.\", 3),\n",
    "    (\"The TechWave X1 is terrible. It's full of glitches and crashes frequently. I regret purchasing it.\", 1),\n",
    "    (\"The TechWave X1 is disappointing. It doesn't live up to the hype and is missing several key functionalities.\", 2)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(reviews_with_ratings, columns=['reviews', 'star_rating'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will convert the text from the reviews into a matrix that creates a column for each word and tracks which reviews contain each word. Take a look at the output below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer for n-grams (single words in this case)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Fit and transform the reviews\n",
    "X = vectorizer.fit_transform(df['reviews'])\n",
    "\n",
    "# Create a new DataFrame with the n-grams\n",
    "ngrams_df = pd.DataFrame(\n",
    "    X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"ngrams_df shape:\", ngrams_df.shape)\n",
    "print(\"ngrams_df columns:\", ngrams_df.columns)\n",
    "\n",
    "# Display the new DataFrame\n",
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "Notice that we have some words that are so common (for example: \"an\", \"and\", \"the\") that they are not useful for our analysis. These words are called stop words. We can remove them by using the stop_words parameter in the CountVectorizer class. Notice the difference in the output below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer for n-grams (single words in this case)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "\n",
    "# Fit and transform the reviews\n",
    "X = vectorizer.fit_transform(df['reviews'])\n",
    "\n",
    "# Create a new DataFrame with the n-grams\n",
    "ngrams_df = pd.DataFrame(\n",
    "    X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"ngrams_df shape:\", ngrams_df.shape)\n",
    "print(\"ngrams_df columns:\", ngrams_df.columns)\n",
    "\n",
    "# Display the new DataFrame\n",
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from 58 n-grams/words to 34 n-grams/words by removing the stop words. This will allow for more meaningful analysis of the reviews.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Length\n",
    "Another modification we may want to make is adjusting how words are grouped. By using single words, we are missing important context such as \"Techwave X1\" being the product name of interest. Let's edit our code to capture bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer for n-grams (single words in this case)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the reviews\n",
    "X = vectorizer.fit_transform(df['reviews'])\n",
    "\n",
    "# Create a new DataFrame with the n-grams\n",
    "ngrams_df = pd.DataFrame(\n",
    "    X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"ngrams_df shape:\", ngrams_df.shape)\n",
    "print(\"ngrams_df columns:\", ngrams_df.columns)\n",
    "\n",
    "# Display the new DataFrame\n",
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Let's edit this a bit more to capture both bigrams and single words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer for n-grams (single words in this case)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the reviews\n",
    "X = vectorizer.fit_transform(df['reviews'])\n",
    "\n",
    "# Create a new DataFrame with the n-grams\n",
    "ngrams_df = pd.DataFrame(\n",
    "    X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"ngrams_df shape:\", ngrams_df.shape)\n",
    "print(\"ngrams_df columns:\", ngrams_df.columns)\n",
    "\n",
    "# Display the new DataFrame\n",
    "ngrams_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You've successfully created n-grams from the reviews using CountVectorizer. You've also learned how to filter out stop words and create n-grams of different lengths. This will be useful when you start building machine learning models to analyze text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Now let's explore another popular technique for text analysis: TF-IDF (Term Frequency-Inverse Document Frequency). TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus. It is often used in information retrieval and text mining. Let's see how it works in practice.\n",
    "\n",
    "TF-IDF combines two metrics:\n",
    "- Term Frequency (TF): How often a word appears in a document\n",
    "- Inverse Document Frequency (IDF): How unique that word is across all documents\n",
    "This helps identify words that are both frequent and meaningful in specific documents.\n",
    "\n",
    "We will first explore this with the social media posts we saw in the slides: \n",
    "\n",
    "- I love learning about text. \n",
    "- Text models are fun. \n",
    "- I love learning at GA. Learning is fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for demo \n",
    "text= [\"I love learning about text.\", \"Text models are fun.\", \"I love learning at GA. Learning is fun!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"tfidf_df shape:\", tfidf_df.shape)\n",
    "print(\"tfidf_df columns:\", tfidf_df.columns)\n",
    "tfidf_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the reviews\n",
    "X = vectorizer.fit_transform(df['reviews'])\n",
    "\n",
    "# Create a new DataFrame with the TF-IDF values\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"tfidf_df shape:\", tfidf_df.shape)\n",
    "print(\"tfidf_df columns:\", tfidf_df.columns)\n",
    "\n",
    "# Display the new DataFrame\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the TF-IDF values are different from the count values. This is because the TF-IDF values are normalized by the term frequency and inverse document frequency, which gives more weight to terms that are rare in the corpus and less weight to terms that are common. This helps to identify the most important terms in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try It! \n",
    "Let's use CountVectorizer and/or TF-IDF to predict the start rating of the reviews for the TechWave X1.\n",
    "\n",
    "We will first import the full data set from `reviews.csv` (`./data/reviews.csv`).  Then we will convert the start rating into a binary outcome (0/1) to approximate sentiment analysis.  Finally, we will build a logistic regression model using the words as features to predict sentiment/ratings. Here we use logistic regression, but other classification models, such as Naive Bayes, Random Forest, or SVM, may perform better. Feel free to explore if you would like extra practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will:\n",
    "1. Process review text using both CountVectorizer and TF-IDF\n",
    "2. Build logistic regression models to predict sentiment\n",
    "3. Compare the performance of both approaches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data and convert the `rating` feature to a numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x1 = pd.read_csv('./data/reviews.csv')\n",
    "df_x1['rating'] = pd.to_numeric(df_x1['rating'])\n",
    "df_x1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create outcome variable from rating \n",
    "\n",
    "You will convert the 5-star rating into a proxy for sentiment where star ratings:  \n",
    "- 1-3: Negative (0)\n",
    "- 4-5: Positive (1)\n",
    "\n",
    "We will do this with a lambda function and `.apply()` and create a new sentiment column that will be used as the outcome in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 5-star ratings to binary sentiment\n",
    "# 1-3: Negative (0), 4-5: Positive (1)\n",
    "\n",
    "df_x1['sentiment'] = df_x1['rating'].apply(lambda x: 0 if x <= 3 else 1)\n",
    "pd.crosstab(df_x1['rating'], df_x1['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a training and test data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_x1['review'], df_x1['sentiment'], test_size=0.2, random_state=1212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process our test with CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a CountVectorizer, fit and transform the training data, and transform the testing data\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_transformed = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a logistic model to predict sentiment using the processed review text\n",
    "\n",
    "Look back at prior lessons for Logistic Regression or checkout the [scikit-learn logistic regression docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "\n",
    "##### Hint!\n",
    "```python\n",
    "# Initialize a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Logistic Regression model, train the model, predict the ratings, evaluate the model using accuracy, and show the confusion matrix.\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the transformed training data\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict the ratings for the testing data\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the process with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_transformed = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Logistic Regression model with TF-IDF processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the transformed training data\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict the ratings for the testing data\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did we observe? \n",
    "\n",
    "CountVectorizer achieved slightly higher accuracy than TF-IDF on our test data. \n",
    "\n",
    "Looking at the confusion matrices:\n",
    "- CountVectorizer correctly identified 2 negative reviews and all positive reviews\n",
    "- TF-IDF classified all reviews as positive\n",
    "\n",
    "This suggests that the raw word frequencies captured by CountVectorizer were more informative for our task than the weighted frequencies from TF-IDF. This makes sense because if someone uses negative words multiple times in a review, that repetition itself may be meaningful for sentiment, rather than something that should be downweighted as is the case with TF-IDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Takeaways\n",
    "- Bag-of-words methods to represent text numerically so that it can be used for by the computer for analysis.\n",
    "- These methods are effective while also being fast and cost effective to implement \n",
    "- These methods don't capture context from the words- if that is required for your task consider using more embeddings instead "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
